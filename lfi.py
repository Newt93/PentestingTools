import requests
from bs4 import BeautifulSoup
import pandas as pd

url_file_path = pd.read_csv("urls.txt")
urls = url_file_path

# A list to store the URLs that have been visited
visited_urls = []

# A list to store the URLs that contain a LFI vulnerability
vulnerable_urls = []

# A list to store the parameters that need to be checked for LFI
parameters_to_check = ["file", "path"]

# A list to store the extensions that need to be checked for LFI
extensions_to_check = pd.read_csv("extentions.txt")

# A list to store the payloads that need to be checked for LFI
payloads = pd.read_csv("payloads.txt")

# Function to read the list of URLs from a text file
def read_urls_from_file(file_path):
    with open(file_path, "r") as file:
        urls = file.read().splitlines()
    return urls

def crawl(urls):
    # Iterate through the URLs
    for url in urls:
        # Add the URL to the list of visited URLs
        visited_urls.append(url)

        # Make a GET request to the URL
        response = requests.get(url)

        # Parse the HTML content of the response
        soup = BeautifulSoup(response.text, "html.parser")

        # Find all the links in the HTML content
        links = soup.find_all("a")

        # Iterate through the links
        for link in links:
            # Get the href attribute of the link
            href = link.get("href")

            # Check if the href is not None and starts with the base URL
            if href and href.startswith(base_url):
                # Check if the href is not in the list of visited URLs
                if href not in visited_urls:
                    # Check if the href contains a parameter that needs to be checked
                    for parameter in parameters_to_check:
                        if parameter in href:
                                    # Check if the href contains an extension that needs to be checked
                            for extension in extensions_to_check:
                                if extension in href:
                                # Add the href to the list of vulnerable URLs
                                    vulnerable_urls.append(href)
                                    break
                    # Perform a GET request with the parameter and extension to check if it's vulnerable
                    for payload in payloads:
                        test_url = href + extension
                        try:
                            test_response = requests.get(test_url, params={parameter: payload})
                            if "root:" in test_response.text:
                                    vulnerable_urls.append(test_url)
                        except:
                            pass
    # Print the list of vulnerable URLs
    print(vulnerable_urls)

# Start the crawling
crawl(urls)


